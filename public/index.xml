<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Jose Torrado</title>
        <link>http://localhost:1313/</link>
        <description>Recent content on Jose Torrado</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Mon, 03 Jun 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Boost Your Pyspark Performance: Best Practices for Data Pipelines</title>
        <link>http://localhost:1313/p/boost-your-pyspark-performance-best-practices-for-data-pipelines/</link>
        <pubDate>Mon, 03 Jun 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/boost-your-pyspark-performance-best-practices-for-data-pipelines/</guid>
        <description>&lt;img src="http://localhost:1313/p/boost-your-pyspark-performance-best-practices-for-data-pipelines/spark-optimization.png" alt="Featured image of post Boost Your Pyspark Performance: Best Practices for Data Pipelines" /&gt;&lt;h1 id=&#34;optimizing-for-spark&#34;&gt;Optimizing for Spark&lt;/h1&gt;
&lt;p&gt;Apache Spark is a powerful tool for big data processing, and PySpark makes it accessible to Python developers. However, to get the best performance out of your PySpark jobs, its a good idea to follow certain best practices.&lt;/p&gt;
&lt;p&gt;I will be going over a few things to keep in mind when developing Pyspark code&lt;/p&gt;
&lt;h2 id=&#34;use-the-dataframe-api-instead-of-rdds&#34;&gt;Use the DataFrame API instead of RDDs&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Prefer using the DataFrame API over RDDs for data processing.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Resilient Distributed Datasets (RDDs) do not provide optimization features like Catalyst optimizer and Tungsten execution engine, leading to inefficient execution plans and slower performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How it Works:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Catalyst Optimizer: This is an extensible query optimizer that automatically optimizes the logical plan of operations. It applies various optimization rules such as predicate pushdown, constant folding, and projection pruning to minimize the amount of data processed and improve performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tungsten Execution Engine: Tungsten improves execution by optimizing memory and CPU usage. It uses off-heap memory for better cache management, and code generation to produce optimized bytecode for execution.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Using DataFrame API&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;spark&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data.csv&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;filtered_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;age&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Instead of RDD&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;rdd&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;textFile&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data.csv&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;filtered_rdd&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rdd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;row&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;row&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;cache-intermediate-results&#34;&gt;Cache Intermediate Results&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Cache intermediate DataFrames that are reused multiple times in your pipeline.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is particularly useful for smaller datasets that are reused multiple times in the code, for uses like mapping (to give an example). Because of Spark&amp;rsquo;s &lt;a class=&#34;link&#34; href=&#34;https://spark.apache.org/docs/latest/rdd-programming-guide#:~:text=All%20transformations%20in%20Spark%20are%20lazy%2C%20in%20that%20they%20do%20not%20compute%20their%20results%20right%20away.&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;lazy evaluation&lt;/a&gt;, any DataFrames that is to be used multiple times but is not cached will essentially be generated again - adding unecessary processing to your code.&lt;/p&gt;
&lt;p&gt;Keep in mind that this data must be stored somewhere though, hence my emphasis on &lt;em&gt;small&lt;/em&gt; datasets. Spark does, however, provide several storage levels (e.g., MEMORY_ONLY, MEMORY_AND_DISK) to control how and where data is cached.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Cache the intermediate DataFrame&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;filtered_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cache&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;filtered_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupBy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;age&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;avoid-shuffling-data&#34;&gt;Avoid Shuffling Data&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Minimize data shuffling by using appropiate partitioning and avoiding wide transformations when possible.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Remember Spark is a distributed computing framework - which means that data is randomly (let&amp;rsquo;s assume random for the sake of this explanation) placed in different &lt;em&gt;executors&lt;/em&gt;, or machines. Whenever an operation requires the same group of rows for an opperation, it needs to find it and move it to the correct machine.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExYTA0ZXd6cGpjNjNxbDZxaWZ4dHExcGM0aTlybDM5NGdubHdwb2YzYyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/QBpkGOjpn3NfVE5tZ4/giphy.gif&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Shuffling data across the network is expensive and can lead to performance bottlenecks. Shuffling occurs during operations like &lt;code&gt;groupBy&lt;/code&gt;, &lt;code&gt;reduceByKey&lt;/code&gt;, and &lt;code&gt;join&lt;/code&gt;, which require data to be redistributed across different nodes.&lt;/p&gt;
&lt;p&gt;By using narrow transformation (like &lt;code&gt;map&lt;/code&gt;, &lt;code&gt;filter&lt;/code&gt;) and appropiate partitioning (e.g. &lt;code&gt;partitioning&lt;/code&gt;, &lt;code&gt;coalesce&lt;/code&gt;), you can reduce the need for shuffling. Narrow transformations operate on a single partition, whereas wide transformations require data from multiple partitions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Repartition the DataFrame to reduce shuffling&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;repartition&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;age&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupBy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;age&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;use-broadcast-variables-for-small-data&#34;&gt;Use Broadcast Variables for Small Data&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Use broadcast variables to efficiently join small datasets with large ones.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Large joins can cause significant shuffling and slow down processing. This happens because each node needs to exchange data with all other nodes, leading to network congestion.&lt;/p&gt;
&lt;p&gt;Broadcast variables allow you to send a read-only copy of a small dataset to all worker nodes. This eliminates the need for shuffling during joins, as each node can directly access the broadcasted data.&lt;/p&gt;
&lt;p&gt;This will only help with small datasets though - broadcasting larger amounts of data can lead to dead executors and network congestion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Broadcast the small DataFrame&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;small_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;spark&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;small_data.csv&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;broadcast_small_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;broadcast&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;small_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;collect&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Where sc is your SparkContext&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Use the broadcast variable in a join&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;id&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;isin&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;row&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;id&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;row&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;broadcast_small_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;optimize-spark-configurations&#34;&gt;Optimize Spark Configurations&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tune Spark configuration parameters to match your workload and cluster resources.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Default configuration settings may not be optimal for your specific workload, leading to inefficient resource utilization and slower performance.&lt;/p&gt;
&lt;p&gt;This, in my experience, can be the most finnicky part of working with Spark. It can feel like endless tunning of knobs to get the configurations just right.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExMndleWlrbHFlOHFmOWJ0NnA4ZGhqdTlvZWh6OWZqMWVucWcyaGh3ZiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/jsrlW09L9Xg2vOwYrB/giphy.gif&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;But it does not need to be a grueling process as long as you understand the basics of what each config is set to do.&lt;/p&gt;
&lt;p&gt;By adjusting parameters such as executor memory, number of cores, and parallelism, you can better utilize your cluster resources and improve job performance.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;spark.executor.memory:&lt;/strong&gt; Amount of memory allocated for each executor.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;spark.executor.cores:&lt;/strong&gt; Number of CPU cores allocated for each executor.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;spark.sql.shuffle.partitions:&lt;/strong&gt; Number of partitions to use when shuffling data for joins or aggregations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are additional configurations for network timeouts, shuffles and a variety of different aspects. Documentation will be your best friend here for debugging, you can find it &lt;a class=&#34;link&#34; href=&#34;https://spark.apache.org/docs/latest/configuration&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Set Spark configuration parameters&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;spark&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;spark.executor.memory&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;4g&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;spark&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;spark.executor.cores&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;2&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;spark&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;spark.sql.shuffle.partitions&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;200&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h1 id=&#34;to-close&#34;&gt;To close&amp;hellip;&lt;/h1&gt;
&lt;p&gt;There are many ways you can optimize your Spark code. Besides these, my general advice would be to always keep in mind how Pyspark will process the data when you are developing your code. By following these best practices, you can significantly improve the performance of your PySpark jobs and ensure that your data pipelines run efficiently.&lt;/p&gt;
&lt;p&gt;Let me know if I missed any of your favorite optimization tips! If you found this post helpful, check out my other blog posts for more insights on data engineering best practices.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Top 5 Common Data Pipeline Pitfalls and How to Avoid Them</title>
        <link>http://localhost:1313/p/top-5-common-data-pipeline-pitfalls-and-how-to-avoid-them/</link>
        <pubDate>Mon, 27 May 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/top-5-common-data-pipeline-pitfalls-and-how-to-avoid-them/</guid>
        <description>&lt;img src="http://localhost:1313/p/top-5-common-data-pipeline-pitfalls-and-how-to-avoid-them/data-pipline.png" alt="Featured image of post Top 5 Common Data Pipeline Pitfalls and How to Avoid Them" /&gt;&lt;h1 id=&#34;theres-always-issues&#34;&gt;There&amp;rsquo;s always issues&amp;hellip;&lt;/h1&gt;
&lt;p&gt;Building and maintaining data pipelines can be a complex task, often facing challenges that affect the performance and reliability of your solutions. I want to cover five common pitfalls you will likely face in data pipeline creation and practical solutions to them.&lt;/p&gt;
&lt;h2 id=&#34;data-duplication&#34;&gt;Data Duplication&lt;/h2&gt;
&lt;p&gt;Data duplication occurs when the same data is ingested, processed and (most importantly) inserted into the final location multiple times. This leads to inconsistent untrustworthy data and inflated storage costs (bad bad).&lt;/p&gt;
&lt;p&gt;How to Avoid:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Create &lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Idempotence&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Idempotent&lt;/a&gt; Pipelines whenever possible:&lt;/strong&gt; Ensure all data processing steps (Extract-Transform-Load) can be safely repeated without creating duplicates.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Implement Deduping Logic:&lt;/strong&gt; Use unique keys or checksums to identify and remove duplicate records.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, when ingesting data from a message queue like &lt;a class=&#34;link&#34; href=&#34;https://torrado.io/p/simplifying-stream-processing-an-introduction-to-pub/sub/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PubSub&lt;/a&gt;, include a unique identifier for each message and use it to check for duplicates before processing.&lt;/p&gt;
&lt;h2 id=&#34;latency-issues&#34;&gt;Latency Issues&lt;/h2&gt;
&lt;p&gt;High latency of data delivery can impact your customer, affecting real-time analytics and decision making. When data is not fresh, it is usually not useful to stakeholders. You want your hard work to have impact.&lt;/p&gt;
&lt;p&gt;How to Avoid:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Optimize Data Processing:&lt;/strong&gt; Use parallel processing adn distributed systems to handle large amounts of data efficiently. Think Apache Spark for example.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Monitor and Tune Performance:&lt;/strong&gt; Regularly monitor resource usage of your data pipeline, Out-of-Memory errors and other possible warning signs. This will give you an idea if your pipeline needs more resources to run faster/more efficiently.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;schema-mismatches&#34;&gt;Schema Mismatches&lt;/h2&gt;
&lt;p&gt;Schema mismatches between different stages of the pipeline can lead to data corruption and processing failures. You also want to guarantee a schema for your downstream consumers, and notify them with ample time when a change is coming.&lt;/p&gt;
&lt;p&gt;How to Avoid:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Schema Registry:&lt;/strong&gt; Use a schema registry to enforce schema consistency and manage schema evolution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Version Control:&lt;/strong&gt; Version your schemas and ensure backward and forward compatibility.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Using git for version control should be standard practice - this will help you keep ALL your code, not just schemas, consistent.&lt;/p&gt;
&lt;h2 id=&#34;no-error-handling&#34;&gt;No Error Handling&lt;/h2&gt;
&lt;p&gt;Not having enough error handling and logging can be a nightmare when trying to troubleshoot pipleine errors. This can also lead to additional data loss if not implemented correclty.&lt;/p&gt;
&lt;p&gt;How to Avoid:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Implement Retry Mechanisms:&lt;/strong&gt; automatically retry pipelines whenever errors are not associated to the pipeline logic itself. Think: not sufficient compute resources - automatic retries will save you time here.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Logging and Alerts:&lt;/strong&gt; Implement comprehensive logging into your code, this will help with troubleshooting. Set up automatic alerts on failures - this can be done with webhooks to Slack for example.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;bad-resource-utilization&#34;&gt;Bad Resource Utilization&lt;/h2&gt;
&lt;p&gt;Cloud compute is expensive - you want to make sure you are getting the most out of it AND paying for only what you need!&lt;/p&gt;
&lt;p&gt;How to Avoid:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Auto-Scaling:&lt;/strong&gt; Memorize this term and make it your best friend. Set auto-scaling to dynamically (and automatically) adjust the resources used based on the workload.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Resource Monitoring:&lt;/strong&gt; Regularly monitor resource usage and optimize configurations. Creating monitoring dashboards to keep track of usage and costs is a great idea.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;therefore-nonetheless&#34;&gt;Therefore, Nonetheless&lt;/h1&gt;
&lt;p&gt;By being aware of these common pitfalls and implementing the suggested solutions, you can build more reliable and efficient data pipelines. Regular monitoring, error handling, and optimization are key to maintaining the health of your data infrastructure.&lt;/p&gt;
&lt;p&gt;Let me know if you&amp;rsquo;d like more details on any of these topics.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Test Note</title>
        <link>http://localhost:1313/test-note/</link>
        <pubDate>Wed, 27 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/test-note/</guid>
        <description>&lt;h1 id=&#34;testing&#34;&gt;Testing&lt;/h1&gt;
&lt;h2 id=&#34;testing-again&#34;&gt;Testing again&lt;/h2&gt;
&lt;p&gt;Hello this is a test&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Simplifying Stream Processing: An Introduction to Pub/Sub</title>
        <link>http://localhost:1313/p/simplifying-stream-processing-an-introduction-to-pub/sub/</link>
        <pubDate>Mon, 18 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/simplifying-stream-processing-an-introduction-to-pub/sub/</guid>
        <description>&lt;img src="http://localhost:1313/p/simplifying-stream-processing-an-introduction-to-pub/sub/pubsub.jpg" alt="Featured image of post Simplifying Stream Processing: An Introduction to Pub/Sub" /&gt;&lt;p&gt;As data engineers, we often find ourselves at the crossroads of data streams that demand real-time processing and insights. Enter Google Cloud Pub/Sub, a fully-managed real-time messaging service that allows you to send and receive messages between independent applications. Think of it as a robust postal service for your data, ensuring that every byte of information is delivered to the right application at the right time.&lt;/p&gt;
&lt;h2 id=&#34;what-is-pubsub-and-how-does-it-fit-into-streaming-pipelines&#34;&gt;What is Pub/Sub, and How Does it Fit into Streaming Pipelines?&lt;/h2&gt;
&lt;p&gt;At its essence, Pub/Sub is about decoupling senders (&lt;code&gt;publishers&lt;/code&gt;) from receivers (&lt;code&gt;subscribers&lt;/code&gt;).
Publishers send messages to a topic, and subscribers listen to that topic, reacting to new messages as they arrive. This is crucial in a streaming pipeline, where live data - like fluctuating Bitcoin prices - needs to be captured, analyzed, and acted upon instantaneously.&lt;/p&gt;
&lt;p&gt;Pub/Sub services fit into streaming pipelines as the data backbone, facilitating seamless data flow between the components of a system. They shine in scenarios where you have data that&amp;rsquo;s constantly being produced and needs immediate attention.&lt;/p&gt;
&lt;h2 id=&#34;lifecycle-of-a-message-in-a-streaming-pipeline-with-pubsub&#34;&gt;Lifecycle of a Message in a Streaming Pipeline with Pubsub&lt;/h2&gt;
&lt;p&gt;To truly grasp the power of Pub/Sub, letâ€™s walk through the lifecycle of a message within a streaming pipeline:&lt;/p&gt;
&lt;h3 id=&#34;production&#34;&gt;Production&lt;/h3&gt;
&lt;p&gt;A message is born when a publisher, say a cryptocurrency exchange, determines the latest Bitcoin price and decides it&amp;rsquo;s time to share it with the world.&lt;/p&gt;
&lt;h3 id=&#34;publication&#34;&gt;Publication&lt;/h3&gt;
&lt;p&gt;This message is wrapped up neatly (serialized) and sent to a Pub/Sub topic. Our topic, bitcoin-price-feed, acts as the mailbox, collecting all outbound messages.&lt;/p&gt;
&lt;h3 id=&#34;delivery&#34;&gt;Delivery&lt;/h3&gt;
&lt;p&gt;Pub/Sub takes over as the diligent postmaster, ensuring that this message is delivered to all subscribers who have expressed interest in bitcoin-price-feed. It&amp;rsquo;s all about the push and pullâ€”subscribers can either choose to receive messages as they come (push) or request them at their own pace (pull).&lt;/p&gt;
&lt;h3 id=&#34;processing&#34;&gt;Processing&lt;/h3&gt;
&lt;p&gt;Subscribers, often stream processors like Google Dataflow, unwrap the message (deserialize) and perform necessary operationsâ€”analyzing trends, triggering alerts, or aggregating data for further analysis.&lt;/p&gt;
&lt;h3 id=&#34;storage-or-action&#34;&gt;Storage or Action&lt;/h3&gt;
&lt;p&gt;Once processed, the message either gets stored for historical analysis or triggers actions in other systems. This could mean updating a live dashboard or adjusting an investment strategy.&lt;/p&gt;
&lt;h3 id=&#34;acknowledgment&#34;&gt;Acknowledgment&lt;/h3&gt;
&lt;p&gt;The final step is the subscriber telling Pub/Sub, &amp;ldquo;Message received and handled.&amp;rdquo; This acknowledgment prevents the same message from being delivered again, keeping the pipeline efficient and clutter-free.&lt;/p&gt;
&lt;h2 id=&#34;an-ally-for-streaming&#34;&gt;An Ally for Streaming&lt;/h2&gt;
&lt;p&gt;You can really consider Pub/Sub an ally that makes real-time data processing less of a daunting task. It&amp;rsquo;s scalable, handling fluctuations in data volume effortlessly. Itâ€™s reliable, ensuring that messages are delivered promptly and accurately. And most importantly, itâ€™s flexible, integrating with a wide array of Google Cloud services and third-party applications.&lt;/p&gt;
&lt;p&gt;In conclusion, Pub/Sub is a pivotal piece of the streaming puzzle, enabling data engineers to build systems that are both responsive and robust. Itâ€™s about bringing order and reliability to the potential chaos of live data streams. As data continues to fuel the decisions of tomorrow, services like Pub/Sub will be the engines that keep the pipelines running smoothly.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Navigating Shiny Object Syndrome in Data Engineering: A Balanced Approach</title>
        <link>http://localhost:1313/p/navigating-shiny-object-syndrome-in-data-engineering-a-balanced-approach/</link>
        <pubDate>Sun, 17 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/navigating-shiny-object-syndrome-in-data-engineering-a-balanced-approach/</guid>
        <description>&lt;img src="http://localhost:1313/p/navigating-shiny-object-syndrome-in-data-engineering-a-balanced-approach/shiny-object.jpeg" alt="Featured image of post Navigating Shiny Object Syndrome in Data Engineering: A Balanced Approach" /&gt;&lt;p&gt;Picture this: you&amp;rsquo;re a kid again, standing in front of the most dazzling candy store imaginable, with every shelf glittering with the promise of the most delectable sweet treat you have ever tasted before.
That&amp;rsquo;s pretty much the life of a tech enthusiast or data engineer in today&amp;rsquo;s world (And to be fair, in any industry).
Shiny Object Syndrome (SOS) isn&amp;rsquo;t just a quirky phraseâ€”it&amp;rsquo;s our daily reality, where the allure of the new and shiny tempts us at every turn.
As someone who&amp;rsquo;s navigated these waters, juggling the excitement of innovation with the steadiness of foundational work, I&amp;rsquo;d like to share a more balanced perspective on this phenomenon.&lt;/p&gt;
&lt;h2 id=&#34;a-dance-with-distraction&#34;&gt;A Dance With Distraction&lt;/h2&gt;
&lt;p&gt;SOS hits hard when the new shiny thing drops into the tech sphere.
Suddenly, it&amp;rsquo;s all anyone can talk about.
AI is the poster child of this syndrome in tech today.
Everywhere you turn, there&amp;rsquo;s a buzz about how AI will revolutionize everything.
Donâ€™t get me wrong; AI is a game-changer, but rushing to adopt AI without having the basics in place is like trying to run before you can walk.
If the foundation isnâ€™t there â€” if the data infrastructure is more of a house of cards â€” then throwing AI into the mix is setting up for a spectacular fall.&lt;/p&gt;
&lt;h2 id=&#34;ai-ml-and-the-echoes-of-past-mistakes&#34;&gt;AI, ML and the Echoes of Past Mistakes&lt;/h2&gt;
&lt;p&gt;The drive towards new technologies often springs from a deep-seated fear of missing out (FOMO).
This isn&amp;rsquo;t just about personal inclinations; it&amp;rsquo;s a collective wave that sweeps through companies and communities, urging us to jump on the bandwagon or be left behind.
But the reality is much different: rushing towards the new without a firm grasp on the essentials leads to a mirage of progress, not real advancement.
Remember the machine learning (ML) frenzy a few years back? Itâ€™s dÃ©jÃ  vu with AI today.
Weâ€™ve been so eager to slap AI onto everything that weâ€™ve forgotten to ask the critical question: do our teams have the quality data and infrastructure AI needs to thrive?
Without these, even the most advanced AI initiatives can flounder, proving the old adage &amp;ldquo;garbage in, garbage out&amp;rdquo; painfully true.&lt;/p&gt;
&lt;h2 id=&#34;from-enthusiasm-to-expertise&#34;&gt;From Enthusiasm to Expertise&lt;/h2&gt;
&lt;p&gt;The journey through the world of data engineering is as much about mastering the tools of today as it is about preparing for the technologies of tomorrow.
The excitement for learning new things is invaluable (an a quality that will take you far in any field), but without application, it&amp;rsquo;s just intellectual collection.
This is where the balance comes into play â€” learning and applying in equal measure.
Moving beyond &lt;a class=&#34;link&#34; href=&#34;https://www.linkedin.com/pulse/how-escape-tutorial-hell-ikechukwu-vincent/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;tutorial hell&lt;/a&gt; requires us to not only absorb new knowledge but also to integrate it into our work in meaningful ways.&lt;/p&gt;
&lt;p&gt;Foucs on just a few of the shiny things you envounter and see that learning to completion - within reason. Learn the technology, consume the tutorials, and most importantly build apps applying what you learned. This will not only cement your learning but it also leaves you with a nice portfolio to show off to employers and colleagues.&lt;/p&gt;
&lt;h2 id=&#34;embracing-the-journey-beyond-the-shiny&#34;&gt;Embracing the Journey: Beyond the Shiny&lt;/h2&gt;
&lt;p&gt;The path forward involves a conscientious approach to innovation. It&amp;rsquo;s about recognizing the value of new technologies like AI, not as trophies to be chased but as tools to be wielded wisely. This means prioritizing the development of a robust data infrastructure and cultivating a deep understanding of the principles underlying our field. By doing so, we can ensure that when we adopt new technologies, we&amp;rsquo;re doing so not for the sake of novelty but for the genuine benefits they bring.&lt;/p&gt;
&lt;h2 id=&#34;wrapping-up&#34;&gt;Wrapping up&lt;/h2&gt;
&lt;p&gt;In conclusion - Shiny Object Syndrome is a siren call in the tech world, luring us with promises of innovation and success.
But true innovation isnâ€™t about chasing the latest trend; itâ€™s about making informed, strategic decisions that build on a solid foundation.
Itâ€™s about learning deeply, applying wisely, and, most importantly, embracing the journey, bumps and all.
As data engineers, our mission extends beyond the allure of new technologies; it&amp;rsquo;s about building sustainable, impactful solutions.
Let&amp;rsquo;s embrace the new, by all means, but let&amp;rsquo;s do so on a bedrock of solid knowledge and practice.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>First Post</title>
        <link>http://localhost:1313/p/first-post/</link>
        <pubDate>Sat, 16 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/first-post/</guid>
        <description>&lt;img src="http://localhost:1313/p/first-post/intro-background.jpg" alt="Featured image of post First Post" /&gt;&lt;p&gt;After a while of debating on it, I have decided to create a blog. My idea is ot make it a compendium of things I have learend over the years, on my self-learning pursuit of becoming a better Engineer - focusing on technical and career development topics. I might eventually delve into other areas of study, depending on how I feel&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;who-am-i&#34;&gt;Who am I?&lt;/h2&gt;
&lt;p&gt;A little more in depth intro than what you will find in the &amp;ldquo;About&amp;rdquo; section - I am Jose and I am originally from Venezuela ðŸ‡»ðŸ‡ª   moved to the US for undergratdaute school, majoring in Mecahnical Engineering.
I figured since I had no clue what I wanted to do after school, ME would be a good generalist discipline to study because of the breadth of the curriculum.&lt;/p&gt;
&lt;p&gt;I took an introduction to Computer Science course as part of my reqs - basically an introduction to a popular language for scientists and engineers called MATLAB&lt;/p&gt;
&lt;p&gt;Afterwards, with a few free hours one semester, I decided to enroll in an additional course, Intro to Python. Probably one of the best decisions I have made to this day. Here I really discovered my interest for &lt;em&gt;coding&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;I was also doing material science research for my department&amp;rsquo;s Engineering Blast and Impact Dynamics lab. For those interested, we were conducting testing on a variety of polymers under high stress-strain conditions using one of the few &lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Split-Hopkinson_pressure_bar&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Split-Hopkinson&lt;/a&gt; pressure bar setups in the country.&lt;/p&gt;
&lt;p&gt;This was my first introduciton to in-depth data anlysis - we were using strain gauge readings to determine the mechanical proeprties of novel composite materials. Pretty cool, co-authored a few scientific papers with the team - some of the coolest friends I have.&lt;/p&gt;
&lt;h2 id=&#34;on-to-data-engineering&#34;&gt;On to Data Engineering&lt;/h2&gt;
&lt;p&gt;After school I decided to pursue a Data Analyst position with the company I interned for during my junior year.
I was worked at a distribtuion center for a very popular e-commerce business, using &lt;strong&gt;all kinds of data&lt;/strong&gt; regarding inventory, order fullfilment, headcounts, etc.
I worked on tons of reporting, data visualizations, basical modeling and honestly pretty advanced SQL queries.
My team here was made up by a bunch of geniuses, and with me being hungry for learning, it was the perfect environment.&lt;/p&gt;
&lt;p&gt;Looking for more technical challenges I moved to a Data Engineer position with an analytics team, and my growth was exponential from that point.
I was now working with data at a very large scale - so I picked up experience in distributed processing frameworks like Spark.
As I gathered more experience I was then involved in more system architecture projects, where I was designing systems that our team would use to generate data for downstream teams/applications.
This involves batch and streaming pipelines, as well as multiple kinds of reports and viz apps.&lt;/p&gt;
&lt;p&gt;I was also given a driver seat in my team&amp;rsquo;s migration from on-prem resources to Google Cloud Platform - this was another multiplier for my growth. I have been able to learn not only the different cloud services, but also how to desing and implement systems that integrate them together. Also been able to pick up expreience with infrastructure management with IaaC and CICD&lt;/p&gt;
&lt;h2 id=&#34;tldr&#34;&gt;TLDR&lt;/h2&gt;
&lt;p&gt;I have discovered that I love learning, and have a knack for making things simple to understand - that&amp;rsquo;s what helps me learn them ðŸ˜†. I also love taking notes, so figured I shared what I learn with others and see if it is helpful. Give back to the community that I have learned so much from! Hope you find something useful out of my notes. Thanks for being here.&lt;/p&gt;
&lt;p&gt;&amp;ndash; Jose&lt;/p&gt;
</description>
        </item>
        <item>
        <title>About</title>
        <link>http://localhost:1313/about/</link>
        <pubDate>Wed, 13 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/about/</guid>
        <description>&lt;p&gt;I am Jose Torrado, a Software Developer with emphasis in Data Engineering based in the US. I have years of experience working with data at various scales and for a variety or purposes. My main area of expertise is Big Data processing - creating batch and streaming data pipelines. I do a good bit of Cloud Architecture design for my job, leveraging Google Cloud Platform (GCP) solutions mostly.&lt;/p&gt;
&lt;p&gt;I love studying and learning new things - pretty much anything. Lately I have been intereseted in the idea of being a &lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Polymath&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;polymath&lt;/a&gt; - the concept really resonates with me so interesting to see there is a name for it! Especially taking hard concepts and making them simple enough for me to understand. I find that quite rewarding.&lt;/p&gt;
&lt;p&gt;The picture, if you are curious, is an DALL-E generated render of my sweet dog Roco - although he looks a lot more thretening in there than he is.&lt;/p&gt;
&lt;p&gt;Teaching is a great form of learning - If you understand something well enough to teach it to others, then you have learened it yourself (see &lt;a class=&#34;link&#34; href=&#34;https://medium.com/taking-note/learning-from-the-feynman-technique-5373014ad230&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Feyman Technique&lt;/a&gt;). That is what I am hoping to achieve here. Share what I learn as I go, maybe it will be helpful for someone else!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The first principle is that you must not fool yourself and you are the easiest person to fool.&amp;quot;&lt;br&gt;
â€” &lt;cite&gt;Richard P. Feynman&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Thanks for being here.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Archives</title>
        <link>http://localhost:1313/archives/</link>
        <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/archives/</guid>
        <description></description>
        </item>
        <item>
        <title>Links</title>
        <link>http://localhost:1313/links/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/links/</guid>
        <description></description>
        </item>
        <item>
        <title>Search</title>
        <link>http://localhost:1313/search/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/search/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>
