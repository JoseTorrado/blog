<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Jose Torrado</title>
        <link>http://localhost:1313/</link>
        <description>Recent content on Jose Torrado</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <lastBuildDate>Fri, 12 Jul 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Setting up a LocalDev Pyspark Environment with JupyterLab</title>
        <link>http://localhost:1313/p/setting-up-a-localdev-pyspark-environment-with-jupyterlab/</link>
        <pubDate>Fri, 12 Jul 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/setting-up-a-localdev-pyspark-environment-with-jupyterlab/</guid>
        <description>&lt;img src="http://localhost:1313/p/setting-up-a-localdev-pyspark-environment-with-jupyterlab/jupyter-spark.png" alt="Featured image of post Setting up a LocalDev Pyspark Environment with JupyterLab" /&gt;&lt;p&gt;Spark is a staple in a data engineering toolset for distributed compute transformations. It does however have a level of overhead and dependencies that need to be sorted in order to set up. These are usually handled by Cloud Services natively (e.g. GCP Dataproc)&lt;/p&gt;
&lt;p&gt;But if you are just getting started with learning Pyspark -
or just want a lightweight and fast way to test some sytax locally,
spinning up a cluster is additional complexity you might not want to deal with
(not to mention it cost money)&lt;/p&gt;
&lt;p&gt;A lot of the tutorials out there are not too intuitve, or are missing some key aspects
that make the development experince easier.
So I decided to compile a list of steps to get you started with pyspark locally quickly.
Hope you find it useful!&lt;/p&gt;
&lt;h2 id=&#34;prereqs&#34;&gt;Prereqs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Python (will be using verison 3.10)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Check if you have python by running&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;python --version
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;install-java&#34;&gt;Install Java&lt;/h2&gt;
&lt;p&gt;Check if you already have it installed&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;java --version
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;If not, go &lt;a class=&#34;link&#34; href=&#34;https://www.java.com/en/download/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt; and follow the installation instructions&lt;/p&gt;
&lt;h2 id=&#34;install-spark&#34;&gt;Install Spark&lt;/h2&gt;
&lt;p&gt;The following commands are for Mac (Bash) - if you are running a bash shell in Windows these should work. Otherwise, they &lt;em&gt;might&lt;/em&gt; need to be slightly altered to run in powershell&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to the &lt;a class=&#34;link&#34; href=&#34;https://spark.apache.org/downloads.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Spark Download&lt;/a&gt; page and get the latest version of spark&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Unzip the downloaded &lt;code&gt;.tgz&lt;/code&gt; file (zipped file)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Move the file to your &lt;code&gt;/opt/&lt;/code&gt; folder in Mac:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo mv spark-3.5.1-bin-hadoop3 /opt/spark-3.5.1
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;In Windows you might not have this folder.
The name or location is not really important as long as you have it somewhere it can&amp;rsquo;t be
accidentally moved or deleted since it will be reference any time you spin up a notebook&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Create a symbolic link to make it easier to switch Spark versions&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you want to install a different version of Spark in the future, you can just adjust the symlink and everything else will still work&amp;hellip;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo ln -s /opt/spark-3.5.1 /opt/spark
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;install-pyspark-and-jupyter&#34;&gt;Install Pyspark and Jupyter&lt;/h2&gt;
&lt;p&gt;To keep things clean it is better to always use virtual environments when installing python modules. This can be done in many ways: conda envs, pipenv, venv, &amp;hellip;&lt;/p&gt;
&lt;p&gt;Here I will be demonstrating with &lt;code&gt;venv&lt;/code&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a new directory to work from: &lt;code&gt;mkdir jupyter-spark&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cd jupyter-spark&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create a virtual environment: &lt;code&gt;python -m venv .pyspark-en&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Activate the virtual environment: &lt;code&gt;source .pyspark-env/bin/activate&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Install pyspark and jupyterlab: &lt;code&gt;pip install pyspark jupyterlab&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;updating-your-shell-rc-file&#34;&gt;Updating your shell rc file&lt;/h2&gt;
&lt;p&gt;Now you need to tell your shell where to find Spark - this is done by setting some environment variables&lt;/p&gt;
&lt;p&gt;This can be done a variety of ways, but to make it as seamless as possible we will be handling it in the &lt;code&gt;~/.bashrc&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: depending on your shell, this step will be different. For example, if you use zsh as your shell, you will need to modify your &lt;code&gt;~/.zshrc&lt;/code&gt; instead&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Add the following environment variables:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# ~~~~~~~~~~~~~~~ Spark ~~~~~~~~~~~~~~~~~~~~~~~~&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Spark&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;SPARK_HOME&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;/opt/spark&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;PATH&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$PATH&lt;/span&gt;:&lt;span class=&#34;nv&#34;&gt;$SPARK_HOME&lt;/span&gt;/bin
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;SPARK_LOCAL_IP&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;127.0.0.1&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Pyspark&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;PYSPARK_DRIVER_PYTHON&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;jupyter
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;PYSPARK_DRIVER_PYTHON_OPTS&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;lab&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;PYSPARK_PYTHON&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;python3
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Save, exit and restart your shell&lt;/p&gt;
&lt;h2 id=&#34;create-a-pyspark-notebook&#34;&gt;Create a Pyspark Notebook&lt;/h2&gt;
&lt;p&gt;Done with the setup, now anytime you want to start your pyspark instance with JupyterLab locally you just need to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;cd into your directory where you installed pyspark&lt;/li&gt;
&lt;li&gt;activate the virtual environment&lt;/li&gt;
&lt;li&gt;run the command &lt;code&gt;pyspark&lt;/code&gt; in your shell&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This will open a JupyterLab instance in your default browser, and you are good to go!&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Scholarship Info</title>
        <link>http://localhost:1313/scholarship-info/</link>
        <pubDate>Thu, 13 Jun 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/scholarship-info/</guid>
        <description>&lt;h1 id=&#34;list-of-schools-and-scholarships-available&#34;&gt;List of Schools and Scholarships Available&lt;/h1&gt;
&lt;p&gt;Let&amp;rsquo;s break it down - if we focus on a handful of schools to apply for scholarships to, it is possible to get the maximum amount&lt;/p&gt;
&lt;h2 id=&#34;overall-approach&#34;&gt;Overall Approach&lt;/h2&gt;
&lt;p&gt;Scholarship application process can be a bit time consuming - each application is likely to ask for one essay at least&lt;/p&gt;
&lt;p&gt;So it is smart to take a holistic approach to your applications:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Apply to 5-10 schools that you want&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Application costs are ~$100 per school so you will want to select carefully&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;For your 2-3 top picks, spend the additional effort to apply to as many scholarships as possible&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;You can base your ranking on whatever, what school is your favorite, possibility of getting more money, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;remember-the-big-picture&#34;&gt;Remember the big picture&lt;/h2&gt;
&lt;p&gt;In my opinion - undergrad prestige is not insanely important. Yes, top companies will recruit heavily from more prestigious schools, but going into debt for this sole reason will ususally not be worth it. High risk.&lt;/p&gt;
&lt;p&gt;You will want to optimize for money first, then for prestige.&lt;/p&gt;
&lt;p&gt;Having good grades in college, good internships and extracurriculars, plus putting in the time for preparing for interviews will make a world of a difference.&lt;/p&gt;
&lt;p&gt;I &lt;strong&gt;CANNOT&lt;/strong&gt; emphasize enough the extracurriculars and internships, &lt;strong&gt;ESPECIALLY&lt;/strong&gt; if going into Comp Sci&lt;/p&gt;
&lt;h2 id=&#34;fasfa&#34;&gt;FASFA&lt;/h2&gt;
&lt;p&gt;Free Application for Federal Student Aid - Being american has multiple benefits, this is one of them. If your family doesn&amp;rsquo;t have the means, you can get free money from the government to go to school. This is SUPER Helpful and its additional AND independent to whatever each school might give you&lt;/p&gt;
&lt;p&gt;You qualify for this.&lt;/p&gt;
&lt;p&gt;It can be up to ~8k a year, so this is something we need to take advantage of&amp;hellip;&lt;/p&gt;
&lt;p&gt;One grant you might apply for is the &lt;a class=&#34;link&#34; href=&#34;https://studentaid.gov/understand-aid/types/grants/pell&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Federal Pell Grant&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-meat-and-potatoes&#34;&gt;The Meat and Potatoes&lt;/h2&gt;
&lt;h3 id=&#34;georgia-tech&#34;&gt;Georgia Tech&lt;/h3&gt;
&lt;p&gt;They offer multiple scholarships for Out Of State students when they complete the &lt;code&gt;Georgia Tech Application for Scholarships and Financial Aid&lt;/code&gt;. You are automatically considered for all by filling this applciation, although criteria for each will vary&lt;/p&gt;
&lt;p&gt;You NEED to apply before the &lt;code&gt;Priority Deadline&lt;/code&gt; for all schools if possible to maximize amount of money they will give you:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Link: &lt;a class=&#34;link&#34; href=&#34;https://finaid.gatech.edu/apply/fall-first-year&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://finaid.gatech.edu/apply/fall-first-year&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Priority Deadline - Jan 31st 2024 for Students of Fall 2024. IN your case you would be a Fall 2025 student, so likely the deadline will be Jan 31st 2025, although it hasn&amp;rsquo;t been publicly posted yet&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;institutional-scholarships&#34;&gt;Institutional Scholarships&lt;/h4&gt;
&lt;p&gt;These basically are scholarships that come from Georgia Tech themselves&amp;hellip;&lt;/p&gt;
&lt;p&gt;Link: &lt;a class=&#34;link&#34; href=&#34;https://finaid.gatech.edu/undergraduate-types-aid/scholarships/institutional-scholarships&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://finaid.gatech.edu/undergraduate-types-aid/scholarships/institutional-scholarships&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Stamps Scholar - Full Ride, they pay for everything. You apply and there is an interview process but I am confident you could get this&lt;/li&gt;
&lt;li&gt;Gold Scholar - Same process as the one above, but they do not state if it is a full ride. However it is a lot of money. If you dont get the one above you could get this one&lt;/li&gt;
&lt;li&gt;Provost Scholar - They give you in state tuition rate. Pretty good if you stack it with other grants/money&lt;/li&gt;
&lt;li&gt;Godbold Scholarship - Need SAT of 1500 or higher and financial need. This one seems good and you could qualify&lt;/li&gt;
&lt;li&gt;BlackRock Scholarship - This one is awesome, but they grant only 2. Also it seems to be AFTER you complete your first year but basically covers everything for the remaining years, PLUS they give you an internship with BlackRock (youd make so much money)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;outside-scholarships&#34;&gt;Outside Scholarships&lt;/h3&gt;
&lt;p&gt;You can get many scholarships not assocaited to any school - Your highschool counselor will be your best friend to help you with this&lt;/p&gt;
&lt;p&gt;You need to speak to them to see what opportunities are available for your school: this could mean a couple extra grand a year!&lt;/p&gt;
&lt;p&gt;Georgia Tech provides these links to look into: &lt;a class=&#34;link&#34; href=&#34;https://finaid.gatech.edu/undergraduate-types-aid/outside-scholarships/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://finaid.gatech.edu/undergraduate-types-aid/outside-scholarships/&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.jkcf.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.jkcf.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>How to get started with Airflow for Data Engineering</title>
        <link>http://localhost:1313/p/how-to-get-started-with-airflow-for-data-engineering/</link>
        <pubDate>Sun, 09 Jun 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/how-to-get-started-with-airflow-for-data-engineering/</guid>
        <description>&lt;img src="http://localhost:1313/p/how-to-get-started-with-airflow-for-data-engineering/blog-cover.png" alt="Featured image of post How to get started with Airflow for Data Engineering" /&gt;&lt;h2 id=&#34;what-is-airflow&#34;&gt;What is Airflow?&lt;/h2&gt;
&lt;p&gt;One of the cornerstones of data engineering is job orchestration. Once you have build your fancy transformation code, that takes data from millions of sources, cleans it and feeds it into your beautiful dimensional model - you need a way to automate it.&lt;/p&gt;
&lt;p&gt;Long gone are the days of bash scripts triggered by a cron schedule, all running on a laptop plugged in 24/7 under your desk at work!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.explainxkcd.com/wiki/images/e/e9/data_pipeline.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;&amp;ldquo;Is the pipeline literally running from your laptop?&amp;rdquo; &amp;ldquo;Don&amp;rsquo;t be silly, my laptop disconnects far too often to host a service we rely on. It&amp;rsquo;s running on my phone.&amp;rdquo;&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://airflow.apache.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Apache Airlfow&lt;/a&gt; is a common orcehstration framework used by data engineers to schedule pipelines. Since it uses Python for the writing of workflows, it makes it very convenient and accessible to developers. Some of the other things it brings to the table:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scalable:&lt;/strong&gt; You are in total control of the workflow, so can be written in a way to scale to really complex scenarios&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Extensible:&lt;/strong&gt; It is extensible by nature, allowing you to build up on the provided Operators and write your own&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Pipeline Generation:&lt;/strong&gt; Pipelines/DAGs are created on te fly upon building the container, this allows for simplication use cases like Templated Workflows&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Monitoring and Logging:&lt;/strong&gt; It comes built in with a dandy logging setup&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Simple Scheduling:&lt;/strong&gt; You can use CRON expressions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open Source:&lt;/strong&gt; Transparency and great support by the community&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is great, and the perfect tool to add to your belt if you are getting started.&lt;/p&gt;
&lt;h2 id=&#34;your-first-pipeline&#34;&gt;Your First Pipeline&lt;/h2&gt;
&lt;h3 id=&#34;setting-up-airflow&#34;&gt;Setting up Airflow&lt;/h3&gt;
&lt;p&gt;We&amp;rsquo;ll keep it simple in this example - however, depending on what you are trying to achieve with your pipeline, the requirements you will need will vary.&lt;/p&gt;
&lt;p&gt;You will need Python. I would also recommend to set up a virtual environment as a best practice (with Python venv for example). Finally, pip install whatever version of airflow you will be using&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Get your airflow module&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install apache-airflow
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Initialize the Airflow DB&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;airflow db init
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;The &lt;code&gt;airflow db&lt;/code&gt; functions as your central repository of DAG states. All of the states and history of DAGs is referenced from here - It serves data to the webserver (UI) and scheduler. Along with the &lt;code&gt;scheduler&lt;/code&gt;, it is the core of airflow operations.&lt;/p&gt;
&lt;h3 id=&#34;creating-a-simple-data-pipeline&#34;&gt;Creating a Simple Data Pipeline&lt;/h3&gt;
&lt;p&gt;All airflow operations are based on &lt;code&gt;Directed Acyclic Graphs&lt;/code&gt; or &lt;code&gt;DAGs&lt;/code&gt;. You can think of DAGs as the building blocks of your pipeline, they contain the instructions of what actions your pipeline will do and in what order. &lt;a class=&#34;link&#34; href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Here&lt;/a&gt; is the official explanation from the docs.&lt;/p&gt;
&lt;p&gt;There are &lt;a class=&#34;link&#34; href=&#34;https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html#declaring-a-dag&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;three&lt;/a&gt; different ways you can define a DAG. Here is an example of what a DAG code would look like - using the standard constructor:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;datetime&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;airflow&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;airflow.operators.dummy&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DummyOperator&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;default_args&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s1&#34;&gt;&amp;#39;owner&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;airflow&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s1&#34;&gt;&amp;#39;start_date&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2024&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s1&#34;&gt;&amp;#39;retries&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;dag&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s1&#34;&gt;&amp;#39;example_dag&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;default_args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;default_args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;An example DAG&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;schedule_interval&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;@daily&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;start&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DummyOperator&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;start&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dag&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;end&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DummyOperator&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;end&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dag&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;start&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;end&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;code&gt;Operators&lt;/code&gt; are the actions your DAGs will do - in this case the &lt;code&gt;DummyOperator&lt;/code&gt; does nothing, I am using it to simply showcase the syntax of a DAG. Operators are what make airflow so useful. There are tons of them and the community support is great.&lt;/p&gt;
&lt;p&gt;Personally, a lot of the pipelines I write use Cloud services from GCP - airflow provides some great GCP operators, see them &lt;a class=&#34;link&#34; href=&#34;https://airflow.apache.org/docs/apache-airflow-providers-google/stable/operators/cloud/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;. They also provide AWS, Azure and lots more.&lt;/p&gt;
&lt;h3 id=&#34;extending-our-example&#34;&gt;Extending our Example&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s create a &lt;code&gt;BashOperator&lt;/code&gt; task for example - it would look something like this if we add it to our DAG above:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;datetime&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;airflow&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;airflow.operators.dummy&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DummyOperator&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;airflow.operators.bash&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BashOperator&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;default_args&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s1&#34;&gt;&amp;#39;owner&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;airflow&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s1&#34;&gt;&amp;#39;start_date&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2024&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s1&#34;&gt;&amp;#39;retries&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;dag&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DAG&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s1&#34;&gt;&amp;#39;example_dag&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;default_args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;default_args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;An example DAG&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;schedule_interval&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;@daily&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;start&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DummyOperator&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;start&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dag&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;t1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BashOperator&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;task_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;print_date&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;bash_command&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;dag&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;end&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DummyOperator&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;task_id&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;end&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dag&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;start&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;end&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;running-your-dag&#34;&gt;Running your DAG&lt;/h3&gt;
&lt;p&gt;The main you will be interacting with your DAGs is through the Airflow UI. To get this locally, you will start the webserver. The following command will spin up the webserver in your localhost port 8080&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;airflow webserver --port &lt;span class=&#34;m&#34;&gt;8080&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;For any scheduling to work, you will then need to run the airflow schduler - run this command&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;airflow scheduler
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Once your run these two, you should be able to access the web UI - it should look somehting like this&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/p/how-to-get-started-with-airflow-for-data-engineering/airflow-local.png&#34;
	width=&#34;770&#34;
	height=&#34;264&#34;
	srcset=&#34;http://localhost:1313/p/how-to-get-started-with-airflow-for-data-engineering/airflow-local_hu9d056f7f96f83356ac66d0277047ef31_37346_480x0_resize_box_3.png 480w, http://localhost:1313/p/how-to-get-started-with-airflow-for-data-engineering/airflow-local_hu9d056f7f96f83356ac66d0277047ef31_37346_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;On your localhost&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;291&#34;
		data-flex-basis=&#34;700px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;things-to-consider&#34;&gt;Things to Consider&amp;hellip;&lt;/h2&gt;
&lt;p&gt;This was a basic overview of what Airflow is and what is can do. There are several things to keep in mind however when we are talking about data engineering production systems.&lt;/p&gt;
&lt;h3 id=&#34;docker&#34;&gt;Docker&lt;/h3&gt;
&lt;p&gt;One standard practice I did not cover here is the use of Docker containers - This makes local development less of a hassle, you are running everything within its own secluded environment in your local machine. It also makes it so that deployments to the cloud are painless - all your dependencies are managed in your Dockerfile&lt;/p&gt;
&lt;p&gt;Docker Compose can also be used to simplify the local dev experience - instead of spinning up several containers and executing separate commands to run the webserver, scehduler, db, etc. you can manage all of it with a single command. Some SaaS companies are basically built on this idea, taking open source software and making it easy to integrate to enterprises. Take a look at &lt;a class=&#34;link&#34; href=&#34;https://www.astronomer.io/?utm_term=astronomer.io&amp;amp;utm_campaign=ch.sem_br.brand_tp.prs_tgt.brand_mt.xct_rgn.namer_lng.eng_dv.all_con.brand-general&amp;amp;utm_source=google&amp;amp;utm_medium=sem&amp;amp;hsa_acc=4274135664&amp;amp;hsa_cam=18419792792&amp;amp;hsa_grp=143270970122&amp;amp;hsa_ad=670851261317&amp;amp;hsa_src=g&amp;amp;hsa_tgt=kwd-1777215822888&amp;amp;hsa_kw=astronomer.io&amp;amp;hsa_mt=e&amp;amp;hsa_net=adwords&amp;amp;hsa_ver=3&amp;amp;gad_source=1&amp;amp;gclid=Cj0KCQjwpZWzBhC0ARIsACvjWRPe8WRLqSZErBu6T8MXOBXoHQQ6Q85kS0P8ZCM7sjF_iIkN-y54PNIaAvsZEALw_wcB&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Astronomer&lt;/a&gt; for example.&lt;/p&gt;
&lt;h3 id=&#34;cloud&#34;&gt;Cloud&lt;/h3&gt;
&lt;p&gt;Leads me to the second topic which is hosting. Airflow will in essence generate an application - which needs resources to run 24/7. Running it locally means that it is still hosted on your laptop - for enterprise and production level solutions this is not enough though.&lt;/p&gt;
&lt;p&gt;It is common practice to have dedicated hardware (either company owned or Cloud platform) that will be running your orchestrator 24/7. This is what enables full automation of your data pipelines.&lt;/p&gt;
&lt;p&gt;In some companies, part of Data Engineering is to do the platform work required to have these stable solutions - where you will dabble into areas of software engineering and DevOps.&lt;/p&gt;
&lt;h2 id=&#34;concluding&#34;&gt;Concluding&lt;/h2&gt;
&lt;p&gt;Apache Airflow can be considered the industry standard for pipeline orchestration. It is an extremely useful and flexible tool for data engineers to create complex data automations. It is also a great introduction to software best practices and even cluster management - because of how it works under the hood.&lt;/p&gt;
&lt;p&gt;Let me know if you would like a more in depth example!&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Boost Your Pyspark Performance: Best Practices for Data Pipelines</title>
        <link>http://localhost:1313/p/boost-your-pyspark-performance-best-practices-for-data-pipelines/</link>
        <pubDate>Mon, 03 Jun 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/boost-your-pyspark-performance-best-practices-for-data-pipelines/</guid>
        <description>&lt;img src="http://localhost:1313/p/boost-your-pyspark-performance-best-practices-for-data-pipelines/spark-optimization.png" alt="Featured image of post Boost Your Pyspark Performance: Best Practices for Data Pipelines" /&gt;&lt;h1 id=&#34;optimizing-for-spark&#34;&gt;Optimizing for Spark&lt;/h1&gt;
&lt;p&gt;Apache Spark is a powerful tool for big data processing, and PySpark makes it accessible to Python developers. However, to get the best performance out of your PySpark jobs, its a good idea to follow certain best practices.&lt;/p&gt;
&lt;p&gt;I will be going over a few things to keep in mind when developing Pyspark code&lt;/p&gt;
&lt;h2 id=&#34;use-the-dataframe-api-instead-of-rdds&#34;&gt;Use the DataFrame API instead of RDDs&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Prefer using the DataFrame API over RDDs for data processing.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Resilient Distributed Datasets (RDDs) do not provide optimization features like Catalyst optimizer and Tungsten execution engine, leading to inefficient execution plans and slower performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How it Works:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Catalyst Optimizer: This is an extensible query optimizer that automatically optimizes the logical plan of operations. It applies various optimization rules such as predicate pushdown, constant folding, and projection pruning to minimize the amount of data processed and improve performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tungsten Execution Engine: Tungsten improves execution by optimizing memory and CPU usage. It uses off-heap memory for better cache management, and code generation to produce optimized bytecode for execution.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Using DataFrame API&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;spark&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data.csv&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;filtered_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;age&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Instead of RDD&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;rdd&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;textFile&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data.csv&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;filtered_rdd&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rdd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;row&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;row&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;cache-intermediate-results&#34;&gt;Cache Intermediate Results&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Cache intermediate DataFrames that are reused multiple times in your pipeline.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is particularly useful for smaller datasets that are reused multiple times in the code, for uses like mapping (to give an example). Because of Spark&amp;rsquo;s &lt;a class=&#34;link&#34; href=&#34;https://spark.apache.org/docs/latest/rdd-programming-guide#:~:text=All%20transformations%20in%20Spark%20are%20lazy%2C%20in%20that%20they%20do%20not%20compute%20their%20results%20right%20away.&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;lazy evaluation&lt;/a&gt;, any DataFrames that is to be used multiple times but is not cached will essentially be generated again - adding unecessary processing to your code.&lt;/p&gt;
&lt;p&gt;Keep in mind that this data must be stored somewhere though, hence my emphasis on &lt;em&gt;small&lt;/em&gt; datasets. Spark does, however, provide several storage levels (e.g., MEMORY_ONLY, MEMORY_AND_DISK) to control how and where data is cached.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Cache the intermediate DataFrame&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;filtered_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cache&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;filtered_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupBy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;age&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;avoid-shuffling-data&#34;&gt;Avoid Shuffling Data&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Minimize data shuffling by using appropiate partitioning and avoiding wide transformations when possible.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Remember Spark is a distributed computing framework - which means that data is randomly (let&amp;rsquo;s assume random for the sake of this explanation) placed in different &lt;em&gt;executors&lt;/em&gt;, or machines. Whenever an operation requires the same group of rows for an opperation, it needs to find it and move it to the correct machine.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExYTA0ZXd6cGpjNjNxbDZxaWZ4dHExcGM0aTlybDM5NGdubHdwb2YzYyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/QBpkGOjpn3NfVE5tZ4/giphy.gif&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Shuffling data across the network is expensive and can lead to performance bottlenecks. Shuffling occurs during operations like &lt;code&gt;groupBy&lt;/code&gt;, &lt;code&gt;reduceByKey&lt;/code&gt;, and &lt;code&gt;join&lt;/code&gt;, which require data to be redistributed across different nodes.&lt;/p&gt;
&lt;p&gt;By using narrow transformation (like &lt;code&gt;map&lt;/code&gt;, &lt;code&gt;filter&lt;/code&gt;) and appropiate partitioning (e.g. &lt;code&gt;partitioning&lt;/code&gt;, &lt;code&gt;coalesce&lt;/code&gt;), you can reduce the need for shuffling. Narrow transformations operate on a single partition, whereas wide transformations require data from multiple partitions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Repartition the DataFrame to reduce shuffling&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;repartition&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;age&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupBy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;age&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;use-broadcast-variables-for-small-data&#34;&gt;Use Broadcast Variables for Small Data&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Use broadcast variables to efficiently join small datasets with large ones.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Large joins can cause significant shuffling and slow down processing. This happens because each node needs to exchange data with all other nodes, leading to network congestion.&lt;/p&gt;
&lt;p&gt;Broadcast variables allow you to send a read-only copy of a small dataset to all worker nodes. This eliminates the need for shuffling during joins, as each node can directly access the broadcasted data.&lt;/p&gt;
&lt;p&gt;This will only help with small datasets though - broadcasting larger amounts of data can lead to dead executors and network congestion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Broadcast the small DataFrame&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;small_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;spark&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;small_data.csv&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;broadcast_small_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;broadcast&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;small_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;collect&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Where sc is your SparkContext&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Use the broadcast variable in a join&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;id&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;isin&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;row&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;id&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;row&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;broadcast_small_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;optimize-spark-configurations&#34;&gt;Optimize Spark Configurations&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tune Spark configuration parameters to match your workload and cluster resources.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Default configuration settings may not be optimal for your specific workload, leading to inefficient resource utilization and slower performance.&lt;/p&gt;
&lt;p&gt;This, in my experience, can be the most finnicky part of working with Spark. It can feel like endless tunning of knobs to get the configurations just right.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExMndleWlrbHFlOHFmOWJ0NnA4ZGhqdTlvZWh6OWZqMWVucWcyaGh3ZiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/jsrlW09L9Xg2vOwYrB/giphy.gif&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;But it does not need to be a grueling process as long as you understand the basics of what each config is set to do.&lt;/p&gt;
&lt;p&gt;By adjusting parameters such as executor memory, number of cores, and parallelism, you can better utilize your cluster resources and improve job performance.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;spark.executor.memory:&lt;/strong&gt; Amount of memory allocated for each executor.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;spark.executor.cores:&lt;/strong&gt; Number of CPU cores allocated for each executor.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;spark.sql.shuffle.partitions:&lt;/strong&gt; Number of partitions to use when shuffling data for joins or aggregations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are additional configurations for network timeouts, shuffles and a variety of different aspects. Documentation will be your best friend here for debugging, you can find it &lt;a class=&#34;link&#34; href=&#34;https://spark.apache.org/docs/latest/configuration&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Set Spark configuration parameters&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;spark&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;spark.executor.memory&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;4g&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;spark&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;spark.executor.cores&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;2&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;spark&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;spark.sql.shuffle.partitions&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;200&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h1 id=&#34;to-close&#34;&gt;To close&amp;hellip;&lt;/h1&gt;
&lt;p&gt;There are many ways you can optimize your Spark code. Besides these, my general advice would be to always keep in mind how Pyspark will process the data when you are developing your code. By following these best practices, you can significantly improve the performance of your PySpark jobs and ensure that your data pipelines run efficiently.&lt;/p&gt;
&lt;p&gt;Let me know if I missed any of your favorite optimization tips! If you found this post helpful, check out my other blog posts for more insights on data engineering best practices.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Top 5 Common Data Pipeline Pitfalls and How to Avoid Them</title>
        <link>http://localhost:1313/p/top-5-common-data-pipeline-pitfalls-and-how-to-avoid-them/</link>
        <pubDate>Mon, 27 May 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/top-5-common-data-pipeline-pitfalls-and-how-to-avoid-them/</guid>
        <description>&lt;img src="http://localhost:1313/p/top-5-common-data-pipeline-pitfalls-and-how-to-avoid-them/data-pipline.png" alt="Featured image of post Top 5 Common Data Pipeline Pitfalls and How to Avoid Them" /&gt;&lt;h1 id=&#34;theres-always-issues&#34;&gt;There&amp;rsquo;s always issues&amp;hellip;&lt;/h1&gt;
&lt;p&gt;Building and maintaining data pipelines can be a complex task, often facing challenges that affect the performance and reliability of your solutions. I want to cover five common pitfalls you will likely face in data pipeline creation and practical solutions to them.&lt;/p&gt;
&lt;h2 id=&#34;data-duplication&#34;&gt;Data Duplication&lt;/h2&gt;
&lt;p&gt;Data duplication occurs when the same data is ingested, processed and (most importantly) inserted into the final location multiple times. This leads to inconsistent untrustworthy data and inflated storage costs (bad bad).&lt;/p&gt;
&lt;p&gt;How to Avoid:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Create &lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Idempotence&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Idempotent&lt;/a&gt; Pipelines whenever possible:&lt;/strong&gt; Ensure all data processing steps (Extract-Transform-Load) can be safely repeated without creating duplicates.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Implement Deduping Logic:&lt;/strong&gt; Use unique keys or checksums to identify and remove duplicate records.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, when ingesting data from a message queue like &lt;a class=&#34;link&#34; href=&#34;https://torrado.io/p/simplifying-stream-processing-an-introduction-to-pub/sub/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PubSub&lt;/a&gt;, include a unique identifier for each message and use it to check for duplicates before processing.&lt;/p&gt;
&lt;h2 id=&#34;latency-issues&#34;&gt;Latency Issues&lt;/h2&gt;
&lt;p&gt;High latency of data delivery can impact your customer, affecting real-time analytics and decision making. When data is not fresh, it is usually not useful to stakeholders. You want your hard work to have impact.&lt;/p&gt;
&lt;p&gt;How to Avoid:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Optimize Data Processing:&lt;/strong&gt; Use parallel processing adn distributed systems to handle large amounts of data efficiently. Think Apache Spark for example.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Monitor and Tune Performance:&lt;/strong&gt; Regularly monitor resource usage of your data pipeline, Out-of-Memory errors and other possible warning signs. This will give you an idea if your pipeline needs more resources to run faster/more efficiently.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;schema-mismatches&#34;&gt;Schema Mismatches&lt;/h2&gt;
&lt;p&gt;Schema mismatches between different stages of the pipeline can lead to data corruption and processing failures. You also want to guarantee a schema for your downstream consumers, and notify them with ample time when a change is coming.&lt;/p&gt;
&lt;p&gt;How to Avoid:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Schema Registry:&lt;/strong&gt; Use a schema registry to enforce schema consistency and manage schema evolution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Version Control:&lt;/strong&gt; Version your schemas and ensure backward and forward compatibility.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Using git for version control should be standard practice - this will help you keep ALL your code, not just schemas, consistent.&lt;/p&gt;
&lt;h2 id=&#34;no-error-handling&#34;&gt;No Error Handling&lt;/h2&gt;
&lt;p&gt;Not having enough error handling and logging can be a nightmare when trying to troubleshoot pipleine errors. This can also lead to additional data loss if not implemented correclty.&lt;/p&gt;
&lt;p&gt;How to Avoid:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Implement Retry Mechanisms:&lt;/strong&gt; automatically retry pipelines whenever errors are not associated to the pipeline logic itself. Think: not sufficient compute resources - automatic retries will save you time here.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Logging and Alerts:&lt;/strong&gt; Implement comprehensive logging into your code, this will help with troubleshooting. Set up automatic alerts on failures - this can be done with webhooks to Slack for example.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;bad-resource-utilization&#34;&gt;Bad Resource Utilization&lt;/h2&gt;
&lt;p&gt;Cloud compute is expensive - you want to make sure you are getting the most out of it AND paying for only what you need!&lt;/p&gt;
&lt;p&gt;How to Avoid:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Auto-Scaling:&lt;/strong&gt; Memorize this term and make it your best friend. Set auto-scaling to dynamically (and automatically) adjust the resources used based on the workload.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Resource Monitoring:&lt;/strong&gt; Regularly monitor resource usage and optimize configurations. Creating monitoring dashboards to keep track of usage and costs is a great idea.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;therefore-nonetheless&#34;&gt;Therefore, Nonetheless&lt;/h1&gt;
&lt;p&gt;By being aware of these common pitfalls and implementing the suggested solutions, you can build more reliable and efficient data pipelines. Regular monitoring, error handling, and optimization are key to maintaining the health of your data infrastructure.&lt;/p&gt;
&lt;p&gt;Let me know if you&amp;rsquo;d like more details on any of these topics.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Test Note</title>
        <link>http://localhost:1313/test-note/</link>
        <pubDate>Wed, 27 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/test-note/</guid>
        <description>&lt;h1 id=&#34;testing&#34;&gt;Testing&lt;/h1&gt;
&lt;h2 id=&#34;testing-again&#34;&gt;Testing again&lt;/h2&gt;
&lt;p&gt;Hello this is a test&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Simplifying Stream Processing: An Introduction to Pub/Sub</title>
        <link>http://localhost:1313/p/simplifying-stream-processing-an-introduction-to-pub/sub/</link>
        <pubDate>Mon, 18 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/simplifying-stream-processing-an-introduction-to-pub/sub/</guid>
        <description>&lt;img src="http://localhost:1313/p/simplifying-stream-processing-an-introduction-to-pub/sub/pubsub.jpg" alt="Featured image of post Simplifying Stream Processing: An Introduction to Pub/Sub" /&gt;&lt;p&gt;As data engineers, we often find ourselves at the crossroads of data streams that demand real-time processing and insights. Enter Google Cloud Pub/Sub, a fully-managed real-time messaging service that allows you to send and receive messages between independent applications. Think of it as a robust postal service for your data, ensuring that every byte of information is delivered to the right application at the right time.&lt;/p&gt;
&lt;h2 id=&#34;what-is-pubsub-and-how-does-it-fit-into-streaming-pipelines&#34;&gt;What is Pub/Sub, and How Does it Fit into Streaming Pipelines?&lt;/h2&gt;
&lt;p&gt;At its essence, Pub/Sub is about decoupling senders (&lt;code&gt;publishers&lt;/code&gt;) from receivers (&lt;code&gt;subscribers&lt;/code&gt;).
Publishers send messages to a topic, and subscribers listen to that topic, reacting to new messages as they arrive. This is crucial in a streaming pipeline, where live data - like fluctuating Bitcoin prices - needs to be captured, analyzed, and acted upon instantaneously.&lt;/p&gt;
&lt;p&gt;Pub/Sub services fit into streaming pipelines as the data backbone, facilitating seamless data flow between the components of a system. They shine in scenarios where you have data that&amp;rsquo;s constantly being produced and needs immediate attention.&lt;/p&gt;
&lt;h2 id=&#34;lifecycle-of-a-message-in-a-streaming-pipeline-with-pubsub&#34;&gt;Lifecycle of a Message in a Streaming Pipeline with Pubsub&lt;/h2&gt;
&lt;p&gt;To truly grasp the power of Pub/Sub, lets walk through the lifecycle of a message within a streaming pipeline:&lt;/p&gt;
&lt;h3 id=&#34;production&#34;&gt;Production&lt;/h3&gt;
&lt;p&gt;A message is born when a publisher, say a cryptocurrency exchange, determines the latest Bitcoin price and decides it&amp;rsquo;s time to share it with the world.&lt;/p&gt;
&lt;h3 id=&#34;publication&#34;&gt;Publication&lt;/h3&gt;
&lt;p&gt;This message is wrapped up neatly (serialized) and sent to a Pub/Sub topic. Our topic, bitcoin-price-feed, acts as the mailbox, collecting all outbound messages.&lt;/p&gt;
&lt;h3 id=&#34;delivery&#34;&gt;Delivery&lt;/h3&gt;
&lt;p&gt;Pub/Sub takes over as the diligent postmaster, ensuring that this message is delivered to all subscribers who have expressed interest in bitcoin-price-feed. It&amp;rsquo;s all about the push and pullsubscribers can either choose to receive messages as they come (push) or request them at their own pace (pull).&lt;/p&gt;
&lt;h3 id=&#34;processing&#34;&gt;Processing&lt;/h3&gt;
&lt;p&gt;Subscribers, often stream processors like Google Dataflow, unwrap the message (deserialize) and perform necessary operationsanalyzing trends, triggering alerts, or aggregating data for further analysis.&lt;/p&gt;
&lt;h3 id=&#34;storage-or-action&#34;&gt;Storage or Action&lt;/h3&gt;
&lt;p&gt;Once processed, the message either gets stored for historical analysis or triggers actions in other systems. This could mean updating a live dashboard or adjusting an investment strategy.&lt;/p&gt;
&lt;h3 id=&#34;acknowledgment&#34;&gt;Acknowledgment&lt;/h3&gt;
&lt;p&gt;The final step is the subscriber telling Pub/Sub, &amp;ldquo;Message received and handled.&amp;rdquo; This acknowledgment prevents the same message from being delivered again, keeping the pipeline efficient and clutter-free.&lt;/p&gt;
&lt;h2 id=&#34;an-ally-for-streaming&#34;&gt;An Ally for Streaming&lt;/h2&gt;
&lt;p&gt;You can really consider Pub/Sub an ally that makes real-time data processing less of a daunting task. It&amp;rsquo;s scalable, handling fluctuations in data volume effortlessly. Its reliable, ensuring that messages are delivered promptly and accurately. And most importantly, its flexible, integrating with a wide array of Google Cloud services and third-party applications.&lt;/p&gt;
&lt;p&gt;In conclusion, Pub/Sub is a pivotal piece of the streaming puzzle, enabling data engineers to build systems that are both responsive and robust. Its about bringing order and reliability to the potential chaos of live data streams. As data continues to fuel the decisions of tomorrow, services like Pub/Sub will be the engines that keep the pipelines running smoothly.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Navigating Shiny Object Syndrome in Data Engineering: A Balanced Approach</title>
        <link>http://localhost:1313/p/navigating-shiny-object-syndrome-in-data-engineering-a-balanced-approach/</link>
        <pubDate>Sun, 17 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/navigating-shiny-object-syndrome-in-data-engineering-a-balanced-approach/</guid>
        <description>&lt;img src="http://localhost:1313/p/navigating-shiny-object-syndrome-in-data-engineering-a-balanced-approach/shiny-object.jpeg" alt="Featured image of post Navigating Shiny Object Syndrome in Data Engineering: A Balanced Approach" /&gt;&lt;p&gt;Picture this: you&amp;rsquo;re a kid again, standing in front of the most dazzling candy store imaginable, with every shelf glittering with the promise of the most delectable sweet treat you have ever tasted before.
That&amp;rsquo;s pretty much the life of a tech enthusiast or data engineer in today&amp;rsquo;s world (And to be fair, in any industry).
Shiny Object Syndrome (SOS) isn&amp;rsquo;t just a quirky phraseit&amp;rsquo;s our daily reality, where the allure of the new and shiny tempts us at every turn.
As someone who&amp;rsquo;s navigated these waters, juggling the excitement of innovation with the steadiness of foundational work, I&amp;rsquo;d like to share a more balanced perspective on this phenomenon.&lt;/p&gt;
&lt;h2 id=&#34;a-dance-with-distraction&#34;&gt;A Dance With Distraction&lt;/h2&gt;
&lt;p&gt;SOS hits hard when the new shiny thing drops into the tech sphere.
Suddenly, it&amp;rsquo;s all anyone can talk about.
AI is the poster child of this syndrome in tech today.
Everywhere you turn, there&amp;rsquo;s a buzz about how AI will revolutionize everything.
Dont get me wrong; AI is a game-changer, but rushing to adopt AI without having the basics in place is like trying to run before you can walk.
If the foundation isnt there  if the data infrastructure is more of a house of cards  then throwing AI into the mix is setting up for a spectacular fall.&lt;/p&gt;
&lt;h2 id=&#34;ai-ml-and-the-echoes-of-past-mistakes&#34;&gt;AI, ML and the Echoes of Past Mistakes&lt;/h2&gt;
&lt;p&gt;The drive towards new technologies often springs from a deep-seated fear of missing out (FOMO).
This isn&amp;rsquo;t just about personal inclinations; it&amp;rsquo;s a collective wave that sweeps through companies and communities, urging us to jump on the bandwagon or be left behind.
But the reality is much different: rushing towards the new without a firm grasp on the essentials leads to a mirage of progress, not real advancement.
Remember the machine learning (ML) frenzy a few years back? Its dj vu with AI today.
Weve been so eager to slap AI onto everything that weve forgotten to ask the critical question: do our teams have the quality data and infrastructure AI needs to thrive?
Without these, even the most advanced AI initiatives can flounder, proving the old adage &amp;ldquo;garbage in, garbage out&amp;rdquo; painfully true.&lt;/p&gt;
&lt;h2 id=&#34;from-enthusiasm-to-expertise&#34;&gt;From Enthusiasm to Expertise&lt;/h2&gt;
&lt;p&gt;The journey through the world of data engineering is as much about mastering the tools of today as it is about preparing for the technologies of tomorrow.
The excitement for learning new things is invaluable (an a quality that will take you far in any field), but without application, it&amp;rsquo;s just intellectual collection.
This is where the balance comes into play  learning and applying in equal measure.
Moving beyond &lt;a class=&#34;link&#34; href=&#34;https://www.linkedin.com/pulse/how-escape-tutorial-hell-ikechukwu-vincent/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;tutorial hell&lt;/a&gt; requires us to not only absorb new knowledge but also to integrate it into our work in meaningful ways.&lt;/p&gt;
&lt;p&gt;Foucs on just a few of the shiny things you envounter and see that learning to completion - within reason. Learn the technology, consume the tutorials, and most importantly build apps applying what you learned. This will not only cement your learning but it also leaves you with a nice portfolio to show off to employers and colleagues.&lt;/p&gt;
&lt;h2 id=&#34;embracing-the-journey-beyond-the-shiny&#34;&gt;Embracing the Journey: Beyond the Shiny&lt;/h2&gt;
&lt;p&gt;The path forward involves a conscientious approach to innovation. It&amp;rsquo;s about recognizing the value of new technologies like AI, not as trophies to be chased but as tools to be wielded wisely. This means prioritizing the development of a robust data infrastructure and cultivating a deep understanding of the principles underlying our field. By doing so, we can ensure that when we adopt new technologies, we&amp;rsquo;re doing so not for the sake of novelty but for the genuine benefits they bring.&lt;/p&gt;
&lt;h2 id=&#34;wrapping-up&#34;&gt;Wrapping up&lt;/h2&gt;
&lt;p&gt;In conclusion - Shiny Object Syndrome is a siren call in the tech world, luring us with promises of innovation and success.
But true innovation isnt about chasing the latest trend; its about making informed, strategic decisions that build on a solid foundation.
Its about learning deeply, applying wisely, and, most importantly, embracing the journey, bumps and all.
As data engineers, our mission extends beyond the allure of new technologies; it&amp;rsquo;s about building sustainable, impactful solutions.
Let&amp;rsquo;s embrace the new, by all means, but let&amp;rsquo;s do so on a bedrock of solid knowledge and practice.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>First Post</title>
        <link>http://localhost:1313/p/first-post/</link>
        <pubDate>Sat, 16 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/first-post/</guid>
        <description>&lt;img src="http://localhost:1313/p/first-post/intro-background.jpg" alt="Featured image of post First Post" /&gt;&lt;p&gt;After a while of debating on it, I have decided to create a blog. My idea is ot make it a compendium of things I have learend over the years, on my self-learning pursuit of becoming a better Engineer - focusing on technical and career development topics. I might eventually delve into other areas of study, depending on how I feel&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;who-am-i&#34;&gt;Who am I?&lt;/h2&gt;
&lt;p&gt;A little more in depth intro than what you will find in the &amp;ldquo;About&amp;rdquo; section - I am Jose and I am originally from Venezuela    moved to the US for undergratdaute school, majoring in Mecahnical Engineering.
I figured since I had no clue what I wanted to do after school, ME would be a good generalist discipline to study because of the breadth of the curriculum.&lt;/p&gt;
&lt;p&gt;I took an introduction to Computer Science course as part of my reqs - basically an introduction to a popular language for scientists and engineers called MATLAB&lt;/p&gt;
&lt;p&gt;Afterwards, with a few free hours one semester, I decided to enroll in an additional course, Intro to Python. Probably one of the best decisions I have made to this day. Here I really discovered my interest for &lt;em&gt;coding&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;I was also doing material science research for my department&amp;rsquo;s Engineering Blast and Impact Dynamics lab. For those interested, we were conducting testing on a variety of polymers under high stress-strain conditions using one of the few &lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Split-Hopkinson_pressure_bar&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Split-Hopkinson&lt;/a&gt; pressure bar setups in the country.&lt;/p&gt;
&lt;p&gt;This was my first introduciton to in-depth data anlysis - we were using strain gauge readings to determine the mechanical proeprties of novel composite materials. Pretty cool, co-authored a few scientific papers with the team - some of the coolest friends I have.&lt;/p&gt;
&lt;h2 id=&#34;on-to-data-engineering&#34;&gt;On to Data Engineering&lt;/h2&gt;
&lt;p&gt;After school I decided to pursue a Data Analyst position with the company I interned for during my junior year.
I was worked at a distribtuion center for a very popular e-commerce business, using &lt;strong&gt;all kinds of data&lt;/strong&gt; regarding inventory, order fullfilment, headcounts, etc.
I worked on tons of reporting, data visualizations, basical modeling and honestly pretty advanced SQL queries.
My team here was made up by a bunch of geniuses, and with me being hungry for learning, it was the perfect environment.&lt;/p&gt;
&lt;p&gt;Looking for more technical challenges I moved to a Data Engineer position with an analytics team, and my growth was exponential from that point.
I was now working with data at a very large scale - so I picked up experience in distributed processing frameworks like Spark.
As I gathered more experience I was then involved in more system architecture projects, where I was designing systems that our team would use to generate data for downstream teams/applications.
This involves batch and streaming pipelines, as well as multiple kinds of reports and viz apps.&lt;/p&gt;
&lt;p&gt;I was also given a driver seat in my team&amp;rsquo;s migration from on-prem resources to Google Cloud Platform - this was another multiplier for my growth. I have been able to learn not only the different cloud services, but also how to desing and implement systems that integrate them together. Also been able to pick up expreience with infrastructure management with IaaC and CICD&lt;/p&gt;
&lt;h2 id=&#34;tldr&#34;&gt;TLDR&lt;/h2&gt;
&lt;p&gt;I have discovered that I love learning, and have a knack for making things simple to understand - that&amp;rsquo;s what helps me learn them . I also love taking notes, so figured I shared what I learn with others and see if it is helpful. Give back to the community that I have learned so much from! Hope you find something useful out of my notes. Thanks for being here.&lt;/p&gt;
&lt;p&gt;&amp;ndash; Jose&lt;/p&gt;
</description>
        </item>
        <item>
        <title>About</title>
        <link>http://localhost:1313/about/</link>
        <pubDate>Wed, 13 Mar 2024 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/about/</guid>
        <description>&lt;p&gt;I am Jose Torrado, a Software Developer with emphasis in Data Engineering based in the US. I have years of experience working with data at various scales and for a variety or purposes. My main area of expertise is Big Data processing - creating batch and streaming data pipelines. I do a good bit of Cloud Architecture design for my job, leveraging Google Cloud Platform (GCP) solutions mostly.&lt;/p&gt;
&lt;p&gt;I love studying and learning new things - pretty much anything. Lately I have been intereseted in the idea of being a &lt;a class=&#34;link&#34; href=&#34;https://en.wikipedia.org/wiki/Polymath&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;polymath&lt;/a&gt; - the concept really resonates with me so interesting to see there is a name for it! Especially taking hard concepts and making them simple enough for me to understand. I find that quite rewarding.&lt;/p&gt;
&lt;p&gt;The picture, if you are curious, is an DALL-E generated render of my sweet dog Roco - although he looks a lot more thretening in there than he is.&lt;/p&gt;
&lt;p&gt;Teaching is a great form of learning - If you understand something well enough to teach it to others, then you have learened it yourself (see &lt;a class=&#34;link&#34; href=&#34;https://medium.com/taking-note/learning-from-the-feynman-technique-5373014ad230&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Feyman Technique&lt;/a&gt;). That is what I am hoping to achieve here. Share what I learn as I go, maybe it will be helpful for someone else!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The first principle is that you must not fool yourself and you are the easiest person to fool.&amp;quot;&lt;br&gt;
 &lt;cite&gt;Richard P. Feynman&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Thanks for being here.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Archives</title>
        <link>http://localhost:1313/archives/</link>
        <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/archives/</guid>
        <description></description>
        </item>
        <item>
        <title>Links</title>
        <link>http://localhost:1313/links/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/links/</guid>
        <description></description>
        </item>
        <item>
        <title>Search</title>
        <link>http://localhost:1313/search/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/search/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>
